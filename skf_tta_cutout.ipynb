{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c31bc-20be-43a2-81b5-a2e26638429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import multiprocessing\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from Cream.TinyViT.models.tiny_vit import tiny_vit_21m_224, tiny_vit_21m_384\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d792a-2385-4bde-be45-0113f2b03125",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch. device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de2fd97-61b3-41be-bf01-ac7da1d66009",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362414ee-cc10-4a9b-9864-30bb6dadb959",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':384,\n",
    "    'EPOCHS':50,\n",
    "    'LEARNING_RATE':1e-4,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151cff4-b45b-4282-97da-5f1145055783",
   "metadata": {},
   "source": [
    "## Fix RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2b985-7950-4582-8a55-e879e38a30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f714e-71d5-496e-aa9e-a26b5fc07f11",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c40fe-435e-4c9c-a00d-6596d81a4e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168f6e1-8907-4791-8ac8-7d2a17bc46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "df['artist'] = le.fit_transform(df['artist'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7527f56-d35e-4045-8af5-fd377e22a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, _, _ = train_test_split(df, df['artist'].values, test_size=0.2, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a543979c-02eb-436d-babe-7649abb2a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sort_values(by=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29ee36-6ffa-4607-9004-db60ce2ec31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = val_df.sort_values(by=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c2ab5-5d6a-474e-8701-19d2911167b4",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8080f89c-ac59-4437-864c-102439fc81be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df, infer=False):\n",
    "    if infer:\n",
    "        return df['img_path'].values\n",
    "    return df['img_path'].values, df['artist'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162536c-103e-4d06-914e-a36713ba7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_paths, train_labels = get_data(train_df)\n",
    "val_img_paths, val_labels = get_data(val_df)\n",
    "all_img_paths, all_labels = get_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e381d720-7439-44db-b37e-9cfcc123d65a",
   "metadata": {},
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c555aab-cabf-4ad4-b519-97622b4b6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, transforms=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_paths[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[index]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99015f92-7b22-489d-b2a2-df2cacaadcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "                            A.Resize(800,800), \n",
    "                            A.RandomCrop(384,384),  # 약 4분의1 size로 crop\n",
    "                            A.Cutout(num_holes=4, max_h_size=32, max_w_size=32, p=0.5),\n",
    "                            A.HorizontalFlip(p=0.5),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "val_transform = A.Compose([\n",
    "                            A.Resize(800,800), \n",
    "                            A.RandomCrop(384,384),  # 약 4분의1 size로 crop\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),  # test와 val transform 분리\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbde5adb-34d1-4975-9179-ce028c64085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_img_paths, train_labels, train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=multiprocessing.cpu_count() // 2)\n",
    "\n",
    "val_dataset = CustomDataset(val_img_paths, val_labels, val_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=multiprocessing.cpu_count() // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fe7c8f-5397-430a-b68a-a8ff057d2ce7",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aeeb81-479c-4897-9e1e-0de8f29f2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes=len(le.classes_)):\n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "        self.backbone = tiny_vit_21m_384(pretrained=True)  # backbone 모델을 tiny_vit384로 설정\n",
    "        \n",
    "        classifier = nn.Linear(576, num_classes)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(classifier.weight)\n",
    "        stdv = 1. / math.sqrt(classifier.weight.size(1))\n",
    "        classifier.bias.data.uniform_(-stdv, stdv)\n",
    "       \n",
    "        self.backbone.head = classifier\n",
    "          \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7faf021-87b4-45d2-ba43-74db63b8d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://discuss.pytorch.org/t/changing-transforms-after-creating-a-dataset/64929/7 참고\n",
    "train과 val에 다른 transformation을 적용하기 위해, 다른 transformation이 적용된 2개의 dataset 생성 후,\n",
    "각 dataset에서 index 이용해, 하나에선 train, 나머지 하나에선 val dataset 생성\n",
    "'''\n",
    "train_transform_dataset = CustomDataset(all_img_paths, all_labels, train_transform)\n",
    "val_transform_dataset = CustomDataset(all_img_paths, all_labels, val_transform)\n",
    "\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_img_paths = get_data(test_df, infer=True)\n",
    "test_dataset = CustomDataset(test_img_paths, None, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=multiprocessing.cpu_count() // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23192719-bbe9-4057-8b9c-9aa9ff2f32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataloader(train_transform_dataset, val_transform_dataset, train_idx, valid_idx, batch_size, num_workers):\n",
    "\n",
    "    # train_transform이 적용된 train_transform_dataset에서 train_idx에 해당하는 Subset 추출\n",
    "    train_set = torch.utils.data.Subset(train_transform_dataset,\n",
    "                                        indices=train_idx)\n",
    "    # val_transform이 적용된 val_transform_dataset에서 valid_idx에 해당하는 Subset 추출\n",
    "    val_set   = torch.utils.data.Subset(val_transform_dataset,\n",
    "                                        indices=valid_idx)\n",
    "    \n",
    "    # 추출된 Train Subset으로 DataLoader 생성\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                              batch_size = batch_size, \n",
    "                              shuffle=True, \n",
    "                              num_workers=num_workers, \n",
    "                              )\n",
    "    # 추출된 Valid Subset으로 DataLoader 생성\n",
    "    val_loader = DataLoader(val_dataset, \n",
    "                            batch_size= batch_size, \n",
    "                            shuffle=False, \n",
    "                            num_workers=num_workers)\n",
    "    \n",
    "    # 생성한 DataLoader 반환\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264204a0-0c23-405e-b9e8-1ff92de44a42",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f6a321-d24f-4156-acd6-7f8df6dddf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stratified_kfold(device):  \n",
    "    n_splits = 7  # 7 fold\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    labels = train_df.artist.to_list()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "\n",
    "    patience = 10  # 10 epoch동안 성능 향상 없을 시, early stopping\n",
    "    oof_pred = None\n",
    "    \n",
    "    for i, (train_idx, valid_idx) in enumerate(skf.split(train_df.img_path.to_list(), labels)):\n",
    "        \n",
    "        num_workers=multiprocessing.cpu_count() // 2\n",
    "        train_loader, val_loader = getDataloader(train_transform_dataset, val_transform_dataset, train_idx, valid_idx, CFG['BATCH_SIZE'], num_workers)\n",
    "            \n",
    "        model = BaseModel()\n",
    "        model.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(params = model.parameters(), lr = 0)\n",
    "        scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=10, T_mult=1, eta_max=0.001,  T_up=3, gamma=0.5)\n",
    "        \n",
    "        best_score = 0\n",
    "        best_model = None\n",
    "        counter = 0\n",
    "        \n",
    "        for epoch in range(1,CFG[\"EPOCHS\"]+1):\n",
    "            model.train()\n",
    "            train_loss = []\n",
    "            for img, label in tqdm(iter(train_loader)):\n",
    "                img, label = img.float().to(device), label.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                model_pred = model(img)\n",
    "\n",
    "                loss = criterion(model_pred, label)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            tr_loss = np.mean(train_loss)\n",
    "\n",
    "            val_loss, val_score = validation(model, criterion, val_loader, device)\n",
    "\n",
    "            print(f'Epoch [{epoch}], Train Loss : [{tr_loss:.5f}] Val Loss : [{val_loss:.5f}] Val F1 Score : [{val_score:.5f}]')\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            if best_score < val_score:\n",
    "                best_model = model\n",
    "                best_score = val_score\n",
    "                counter=0\n",
    "                # 갱신 시마다  best model 저장 -> fold별 마지막 weight이 fold별 best weight\n",
    "                torch.save(model, f\"weights/fold{i}_{epoch:03}_f1score_{val_score:4.2%}.pt\")\n",
    "            else:\n",
    "                counter+=1\n",
    "                \n",
    "            if counter > patience:\n",
    "                print(\"Early Stopping...\")\n",
    "                break\n",
    "           \n",
    "        all_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for images in test_loader:\n",
    "                \n",
    "                images = images.float().to(device)\n",
    "            \n",
    "                pred = best_model(images) / 2 # 원본 이미지를 예측하고\n",
    "                pred += best_model(torch.flip(images, dims=(-1,))) / 2 # horizontal_flip으로 뒤집어 예측하여 누적\n",
    "                all_predictions.extend(pred.cpu().numpy())\n",
    "\n",
    "            fold_pred = np.array(all_predictions)\n",
    "\n",
    "        # OOF\n",
    "        if oof_pred is None:\n",
    "            oof_pred = fold_pred / n_splits\n",
    "        else:\n",
    "            oof_pred += fold_pred / n_splits\n",
    "        \n",
    "        \n",
    "        oof_pred_list = []\n",
    "        if i == n_splits-1:\n",
    "            \n",
    "            # 제출용 csv 생성\n",
    "            oof_pred = torch.from_numpy(oof_pred)\n",
    "            oof_pred_ans = oof_pred.argmax(dim=-1)\n",
    "            oof_pred_ans = oof_pred_ans.detach().cpu().numpy().tolist()\n",
    "            preds = le.inverse_transform(oof_pred_ans) # LabelEncoder로 변환 된 Label을 다시 화가이름으로 변환\n",
    "            submit = pd.read_csv('sample_submission.csv')\n",
    "            submit['artist'] = preds\n",
    "            save_answer_path = './output/stratified_7fold_tta_cutout_answer.csv'\n",
    "            submit.to_csv(save_answer_path, index=False)\n",
    "            \n",
    "            # ensemble을 위한 logit csv 생성\n",
    "            oof_pred = oof_pred.detach().cpu().numpy().tolist()\n",
    "            submit_logit = pd.read_csv('logit_sample_submission.csv')\n",
    "            submit_logit['artist'] = oof_pred\n",
    "            save_answer_path2 = './output/stratified_7fold_tta_cutout_logit.csv'\n",
    "            submit_logit.to_csv(save_answer_path2, index=False)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            print(f\"Inference Done! Inference result saved at {save_answer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0a32f-3c0d-4a89-abd2-a24c246e1c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def competition_metric(true, pred):\n",
    "    return f1_score(true, pred, average=\"macro\")\n",
    "\n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    model_preds = []\n",
    "    true_labels = []\n",
    "    \n",
    "    val_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, label in tqdm(iter(val_loader)):\n",
    "            img, label = img.float().to(device), label.to(device)\n",
    "            \n",
    "            model_pred = model(img)\n",
    "            \n",
    "            loss = criterion(model_pred, label)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += label.detach().cpu().numpy().tolist()\n",
    "        \n",
    "    val_f1 = competition_metric(true_labels, model_preds)\n",
    "    return np.mean(val_loss), val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df86157-a09e-476b-80b4-cc370fd29fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmUpRestarts(lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr) * self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (\n",
    "                        1 + math.cos(math.pi * (self.T_cur - self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "\n",
    "        self.eta_max = self.base_eta_max * (self.gamma ** self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd1cb06-54ab-4d30-9f3d-3d4d149a38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2165f7ea-1ccf-4ca9-9b03-fa8996e30be5",
   "metadata": {},
   "source": [
    "## Run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f0616-0cf0-4c44-925d-929377aa1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel()\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = 0)\n",
    "scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=10, T_mult=1, eta_max=0.001,  T_up=3, gamma=0.5)\n",
    "train_stratified_kfold(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
